{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "liarplus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smara97/FakeNews/blob/master/liarplus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg8QmECF_eHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7493e84d-eec8-437c-bed7-ff92b3f6461a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from google.colab import files,drive\n",
        "import collections\n",
        "import nltk\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from torch.autograd import Variable\n",
        "from string import punctuation\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k39R3AAAADa8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a1d406e-a15e-4569-dfd0-aeed87d8c46e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpKF7Bj6Hw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r',encoding='UTF-8') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_YGt1t36NVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    vocab_len = len(word_to_index) + 1\n",
        "    emb_matrix = np.zeros((vocab_len,300))\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "    return emb_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG5u2iTih9ID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(text):\n",
        "  text=text.lower()\n",
        "  stp=set(stopwords.words(\"english\"))\n",
        "  placesp = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "  removech= re.compile('[^0-9a-z #+_]')\n",
        "  st=WordNetLemmatizer()\n",
        "  text=re.sub(placesp,' ',text)\n",
        "  text=re.sub(removech,' ',text)\n",
        "  text=text.split()\n",
        "  text=[w for w in text if not w in stp]\n",
        "  text=[st.lemmatize(w) for w in text]\n",
        "  text=\" \".join(text)\n",
        "  text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyXAjL26LJxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transfer_sent(text,word_to_index):\n",
        "  text=text.split(' ')\n",
        "  ret=[]\n",
        "  for w in text:\n",
        "    if w in word_to_index and w !=\"\":\n",
        "      ret.append(word_to_index[w])\n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAFkt1gUUIkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simlarity(dim1,dim2):\n",
        "  return (torch.dot(dim1,dim2)/(torch.sqrt(torch.sum(dim1**2))*torch.sqrt(torch.sum(dim2**2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsZ8lFKziZh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def round(pred):\n",
        "  for i in range(len(pred)):\n",
        "    if pred[i]>=0.5:\n",
        "      pred[i]=1\n",
        "    else:\n",
        "      pred[i]=0\n",
        "  return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owt9-oD4iutn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Accu(pred,labels):\n",
        "  ret=0\n",
        "  for i in range(len(labels)):\n",
        "    if pred[i]==labels[i]:\n",
        "      ret+=1\n",
        "  return ret/len(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rSODM6EKsue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_tensor(training):\n",
        "  for i in range(len(training)):\n",
        "    training[i][0]=torch.from_numpy(training[i][0])\n",
        "    training[i][1]=torch.from_numpy(training[i][1])\n",
        "    training[i][2]=torch.from_numpy(training[i][2])\n",
        "    training[i]=torch.cat([training[i][0],training[i][1],training[i][2]])\n",
        "  return training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N6FmfbWRwwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retmax(dftrain):\n",
        "\n",
        "  stmax,sumax,jumax=0,0,0\n",
        "  for i in range(dftrain.shape[0]):\n",
        "\n",
        "    stmax=max(stmax,len(np.array(dftrain.loc[i,'statement'])))\n",
        "\n",
        "    sumax=max(sumax,len(np.array(dftrain.loc[i,'subject'])))\n",
        "\n",
        "    jumax=max(jumax,len(np.array(dftrain.loc[i,'justification'])))\n",
        "\n",
        "  return stmax,sumax,jumax\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GG5byD7ZbBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert2D(Xs,max_lens):\n",
        "   X_indices = np.zeros((Xs[0].shape[0], sum(max_lens)))\n",
        "   pls=0\n",
        "\n",
        "   for i in range(Xs[0].shape[0]):\n",
        "     pls=0\n",
        "     \n",
        "     for j in range(0,len(Xs[0][i])):\n",
        "       X_indices[i][j+pls]=Xs[0][i][j]\n",
        "     pls=max_lens[0]\n",
        "\n",
        "     for j in range(0,len(Xs[1][i])):\n",
        "       X_indices[i][j+pls+1]=Xs[1][i][j]\n",
        "     pls=max_lens[1]+max_lens[0]\n",
        "\n",
        "     for j in range(0,len(Xs[2][i])):\n",
        "       X_indices[i][j+pls+1]=Xs[2][i][j]\n",
        "   return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C7Ej5cQNHUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convertlabel = {\n",
        "\t'pants-fire': 0,\n",
        "\t'false': 0,\n",
        "\t'barely-true': 0,\n",
        "\t'half-true': 1,\n",
        "\t'mostly-true': 1,\n",
        "\t'true': 1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvDjhEJ5AuqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols=['index','ID','label','statement','subject','speaker',\n",
        "      'speaker_job','state','party','barely_true',\n",
        "      'false','half_true','mostly_true','pants_on_fire',\n",
        "      'context','justification']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmu1XWkeJ3gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"/content/drive/My Drive/Datasets/Word Embedding/glove.6B.300d.txt\")\n",
        "word_embedding=pretrained_embedding_layer(word_to_vec_map, word_to_index)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI1wca7kAHbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/train.tsv\",sep=\"\\t\",header=None)\n",
        "dfval=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/val.tsv\",sep=\"\\t\",header=None)\n",
        "dftest=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/test.tsv\",sep=\"\\t\",header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8jZpxs8CIrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain.columns=cols\n",
        "dfval.columns=cols\n",
        "dftest.columns=cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCo9SEQ6SiQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain=dftrain.loc[:,['statement','subject','justification','label']]\n",
        "dfval=dfval.loc[:,['statement','subject','justification','label']]\n",
        "dftest=dftest.loc[:,['statement','subject','justification','label']]\n",
        "dftrain=dftrain.append(dfval)\n",
        "dftrain=dftrain.append(dftest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiytTGihAnEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain=dftrain.dropna(axis=0)\n",
        "\n",
        "dftrain=dftrain.reset_index()\n",
        "\n",
        "dftrain=dftrain.drop(['index'],axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlwxZGVZMi54",
        "colab_type": "code",
        "outputId": "cb0e3a6e-be08-4662-c64b-e332bfe85f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "dftrain.head(2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>justification</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>That's a premise that he fails to back up. Ann...</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>Surovell said the decline of coal \"started whe...</td>\n",
              "      <td>half-true</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           statement  ...      label\n",
              "0  Says the Annies List political group supports ...  ...      false\n",
              "1  When did the decline of coal start? It started...  ...  half-true\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHJmQW1XNO-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(dftrain.shape[0]):\n",
        "  dftrain.loc[i,'label']=convertlabel[dftrain.loc[i,'label']]\n",
        "  dftrain.loc[i,'statement']=transfer_sent(clean(dftrain.loc[i,'statement']),word_to_index)\n",
        "  dftrain.loc[i,'subject']=transfer_sent(clean(dftrain.loc[i,'subject']),word_to_index)\n",
        "  dftrain.loc[i,'justification']=transfer_sent(clean(dftrain.loc[i,'justification']),word_to_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ2rer47LEc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrainy=dftrain['label']\n",
        "\n",
        "dftrain=dftrain.drop(['label'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP-MvvSQU45O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stmax,sumax,jumax=retmax(dftrain)\n",
        "Fulldata=np.array(convert2D([dftrain.statement,dftrain.subject,dftrain.justification],[410,30,1000]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGjc_T6CrFAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrainy=list(dftrainy)\n",
        "training,trainingy=Fulldata[:10153],dftrainy[:10153]\n",
        "validation,validationy=Fulldata[10154:11422],dftrainy[10154:11422]\n",
        "testing,testingy=Fulldata[11423:],dftrainy[11423:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bInMSM2trKpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training=torch.from_numpy(training)\n",
        "trainingy = torch.tensor(trainingy) \n",
        "train_tensor = torch.utils.data.TensorDataset(training, trainingy)\n",
        "\n",
        "validation=torch.from_numpy(validation)\n",
        "validationy = torch.tensor(validationy) \n",
        "valid_tensor = torch.utils.data.TensorDataset(validation, validationy)\n",
        "\n",
        "testing=torch.from_numpy(testing)\n",
        "testingy = torch.tensor(testingy) \n",
        "test_tensor = torch.utils.data.TensorDataset(testing, testingy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT2PBtkUwPLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader=torch.utils.data.DataLoader(dataset=train_tensor,batch_size=64,shuffle=True, num_workers=0)\n",
        "vali_loader=torch.utils.data.DataLoader(dataset=valid_tensor,batch_size=64,shuffle=True, num_workers=0)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_tensor,batch_size=64,shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgSrAHmoymZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "\n",
        "    super(NN, self).__init__()\n",
        "\n",
        "        \n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)   \n",
        "\n",
        "    self.fc = nn.Linear(903, 256)\n",
        "    self.relu1=nn.ReLU()\n",
        "    self.dropout1=nn.Dropout(0.4)\n",
        "\n",
        "    self.fc2=nn.Linear(256,1)\n",
        "    self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    x = self.conv(x)\n",
        "\n",
        "    out = self.dropout1(self.relu1(self.fc(x)))\n",
        "    out=self.sig(self.fc2(out))\n",
        "    return out\n",
        "    \n",
        "  def conv(self,x):\n",
        "    batch=len(x)\n",
        "    ret=torch.zeros((batch,903)).cuda()\n",
        "    st=torch.zeros(300).cuda()\n",
        "    su=torch.zeros(300).cuda()\n",
        "    ju=torch.zeros(300).cuda()\n",
        "\n",
        "    for i in range(len(x)):\n",
        "      cnt=0\n",
        "      for j in range(0,411):\n",
        "        if (x[i][j]!=0):\n",
        "          cnt+=1\n",
        "          st+=self.embedding(x[i][j])\n",
        "      st/=cnt\n",
        "      cnt=0\n",
        "      for j in range(411,441):\n",
        "        if (x[i][j]!=0):\n",
        "          cnt+=1\n",
        "          su+=self.embedding(x[i][j])\n",
        "      su/=cnt\n",
        "\n",
        "      cnt=0\n",
        "      for j in range(411,1440):\n",
        "        if (x[i][j]!=0):\n",
        "          cnt+=1\n",
        "          ju+=self.embedding(x[i][j])\n",
        "      ju/=cnt\n",
        "\n",
        "      indx=0\n",
        "      for j in range(len(st)):\n",
        "        ret[i][indx]=st[j]\n",
        "        indx+=1\n",
        "      ret[i][indx]=simlarity(st,su)\n",
        "      indx+=1\n",
        "      for j in range(len(su)):\n",
        "        ret[i][indx]=su[j]\n",
        "        indx+=1\n",
        "\n",
        "      ret[i][indx]=simlarity(su,ju)\n",
        "      indx+=1\n",
        "\n",
        "      for j in range(len(ju)):\n",
        "        ret[i][indx]=ju[j]\n",
        "        indx+=1\n",
        "\n",
        "      ret[i][indx]=simlarity(st,ju)\n",
        "    return ret\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2kIv7YUymWl",
        "colab_type": "code",
        "outputId": "5cc94e86-2178-4bc0-88e1-5d9198569f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "embedding_dim = 300\n",
        "net = NN(vocab_size, embedding_dim)\n",
        "print(net)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN(\n",
            "  (embedding): Embedding(400001, 300)\n",
            "  (fc): Linear(in_features=903, out_features=256, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (dropout1): Dropout(p=0.4, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ewj9UOVymBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=0.01\n",
        "\n",
        "net.cuda()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1XfvzUylwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4\n",
        "net.train()\n",
        "for e in range(epochs):\n",
        "  Taccuracy,Vaccuracy=[],[]\n",
        "  losses=[]\n",
        "  \n",
        "  for inputs, labels in train_loader:\n",
        "\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "   \n",
        "    net.zero_grad()\n",
        "    \n",
        "    output= net(inputs.long())\n",
        "    Taccuracy.append(Accu(torch.round(output),labels))\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    loss = criterion(output.squeeze().float(), labels.float())\n",
        "    loss = Variable(loss, requires_grad = True)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: {}/{}...\".format(e+1, epochs),\"Training Finished\")\n",
        "  with torch.no_grad():\n",
        "\n",
        "    net.eval()\n",
        "    val_losses = []\n",
        "  \n",
        "    for inputs, labels in vali_loader:\n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "      output= net(inputs.long())\n",
        "      Vaccuracy.append(Accu(torch.round(output),labels))\n",
        "      val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "      val_losses.append(val_loss.item())\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "          \"Tarining Loss: {:.6f}...\".format(np.mean(losses)),\n",
        "          \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "          \"Val Accu:{:.6f}\".format(np.mean(Vaccuracy)),\n",
        "          \"Training Accu:{:.6f}\".format(np.mean(Taccuracy)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1a3Oruchs8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex,lb=next(iter(vali_loader))\n",
        "net.eval()\n",
        "out=net(ex.cuda().long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTGRPwkHeHH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fe0acbf6-8f79-4341-e975-7f3f5ba23e7e"
      },
      "source": [
        "print(out.squeeze())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.5246, 0.5155, 0.5633, 0.5441, 0.5258, 0.5108, 0.5207, 0.5465, 0.5531,\n",
            "        0.5472, 0.5256, 0.5301, 0.5138, 0.5189, 0.5227, 0.5230, 0.5166, 0.4974,\n",
            "        0.5391, 0.5183, 0.5123, 0.5129, 0.5245, 0.5396, 0.5468, 0.5251, 0.5352,\n",
            "        0.5359, 0.5259, 0.4777, 0.5189, 0.4743], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ5F0J26eaeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loo=criterion(out.squeeze(),lb.cuda().float())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4AGhSRnetin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a071028e-a800-4b0f-aef1-cf6d0e46f65e"
      },
      "source": [
        "loo.item()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6827772259712219"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHhL5N2zfBWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}