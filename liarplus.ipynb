{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "liarplus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smara97/FakeNews/blob/master/liarplus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg8QmECF_eHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ab5220be-4c4f-496a-f3b8-c770f635cf8a"
      },
      "source": [
        "from google.colab import files,drive\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from torch.autograd import Variable\n",
        "from string import punctuation\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import nltk\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k39R3AAAADa8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33bd50ed-1548-4959-87b9-ac13b9659419"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3HBqw4Onjsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Model to Training Data \n",
        "\n",
        "  Forward Function\n",
        "Take length of Embedding and Dim of it and Create Embedding Layer by torch Framework\n",
        "Create Neural Network Layer take 903 input and return 256 as output layer\n",
        "then bass output to non-activation function layer then add dropout to output\n",
        "bass output to Linear take 256 and return number of class 1 and then add sigmoid to output\n",
        "\n",
        "  Conv Function \n",
        "Take inputs ( self, inputs layer[batch size of training inputs*Featuers] (64,1440) )\n",
        "\n",
        "conv inputs layer from [64,1440] to [64,903]\n",
        "\n",
        "first 300s number repersent the vector sentence(Statement) of Embedding\n",
        "\n",
        "301 add Similarity of Statement Featuer (first [410] numbers of orignal input) and \n",
        "  Subject Featuer (second [30] numbers of orignal input)\n",
        "\n",
        "from 302 to 602 add number repersent the vector sentence(Subject) of Embedding\n",
        "\n",
        "602 add Similarity of Subject Featuer ([30] numbers of orignal input) and \n",
        "  Justification Featuer (second [1000] numbers of orignal input)\n",
        "\n",
        "from 602 to 902 add number repersent the vector sentence(Justifaction) of Embedding\n",
        "\n",
        "903 add Similarity of Justification Featuer ([100] numbers of orignal input) and \n",
        "  Statement Featuer (second [410] numbers of orignal input)\n",
        "\n",
        "and return [64,903]\n",
        "\n",
        "Then Pass output of Conv to Forward\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class NN(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim,word_embedding):\n",
        "\n",
        "    super(NN, self).__init__()\n",
        "\n",
        "        \n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)   \n",
        "    self.embedding.weight.data.copy_(word_embedding)\n",
        "    self.lstm = nn.LSTM(903, 128, 2, dropout=0.6, batch_first=True)\n",
        "    self.fc = nn.Linear(128,1)\n",
        "    self.dropout=nn.Dropout(0.5)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x,hidden):\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    x = self.conv(x)\n",
        "    \n",
        "\n",
        "    lstm_out, hidden = self.lstm(x, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1,128)\n",
        "    out = self.dropout(lstm_out)\n",
        "    out=self.sig(self.fc(out))\n",
        "    sig_out = out.view(batch_size, -1)\n",
        "    sig_out = sig_out[:, -1]\n",
        "    return sig_out, hidden\n",
        "    \n",
        "  def conv(self,x):\n",
        "    batch=len(x)\n",
        "\n",
        "    ret=torch.zeros((batch,903)).cuda()\n",
        "    st=torch.zeros(300).cuda()\n",
        "    su=torch.zeros(300).cuda()\n",
        "    ju=torch.zeros(300).cuda()\n",
        "\n",
        "\n",
        "    \n",
        "    for i in range(batch):\n",
        "  \n",
        "      st=self.embedding(x[i][0:411]).sum(dim=0)/(x[i][0:411]!=0).sum()\n",
        "      su=self.embedding(x[i][411:441]).sum(dim=0)/(x[i][411:441]!=0).sum()\n",
        "      ju=self.embedding(x[i][441:1440]).sum(dim=0)/(x[i][441:1440]!=0).sum()\n",
        "      \n",
        "      ret[i][:300]=st\n",
        "      ret[i][300]=simlarity(st,su)\n",
        "\n",
        "      ret[i][301:601]=su\n",
        "      ret[i][601]=simlarity(su,ju)\n",
        "\n",
        "      ret[i][602:902]=ju\n",
        "      \n",
        "      ret[i][902]=simlarity(st,ju)\n",
        "    return ret.view(1,batch,903)\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(2, 1, 128).zero_().cuda(),\n",
        "                  weight.new(2, 1, 128).zero_().cuda())\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpKF7Bj6Hw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        " Read Glove File take url of file return the two dictionaries ( word to index and word to vector in embedding )\n",
        " and one list of index to word  \n",
        " (glove file url) --> words_to_index, index_to_words, word_to_vec_map\n",
        " \n",
        " \"\"\"\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r',encoding='UTF-8') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_YGt1t36NVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Word Embeddings of words take dictionary of word to embedding and word to index\n",
        "and return Embeddings Matrix [index,Embedding] \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    vocab_len = len(word_to_index) + 1\n",
        "    emb_matrix = np.zeros((vocab_len,300))\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "    return emb_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG5u2iTih9ID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Clean Text \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def clean(text):\n",
        "  text=text.lower()\n",
        "  stp=set(stopwords.words(\"english\"))\n",
        "  placesp = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "  removech= re.compile('[^0-9a-z #+_]')\n",
        "  st=WordNetLemmatizer()\n",
        "  text=re.sub(placesp,' ',text)\n",
        "  text=re.sub(removech,' ',text)\n",
        "  text=text.split()\n",
        "  text=[w for w in text if not w in stp]\n",
        "  text=[st.lemmatize(w) for w in text]\n",
        "  text=\" \".join(text)\n",
        "  text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyXAjL26LJxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Transfer sentence to indeces word in Embedding\n",
        "take text and word to index dictionary \n",
        "return list of indeces word in Embedding\n",
        "\n",
        "\"\"\"\n",
        "def transfer_sent(text,word_to_index):\n",
        "  text=text.split(' ')\n",
        "  ret=[]\n",
        "  for w in text:\n",
        "    if w in word_to_index and w !=\"\":\n",
        "      ret.append(word_to_index[w])\n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAFkt1gUUIkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Similarity of two Documnets \n",
        "take two documnets\n",
        "return The Similarity of documents\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def simlarity(dim1,dim2):\n",
        "  return (torch.dot(dim1,dim2)/(torch.sqrt(torch.sum(dim1**2))*torch.sqrt(torch.sum(dim2**2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owt9-oD4iutn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Accuracy of predict labels\n",
        "take predict labels and target labels\n",
        "return number of accept label in predict labels\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def Accu(pred,labels):\n",
        "  ret=0\n",
        "  for i in range(len(labels)):\n",
        "    if pred[i]==labels[i]:\n",
        "      ret+=1\n",
        "  return ret/len(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N6FmfbWRwwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "calculate the Max Length in every column in Data Frame \n",
        "take Data Frame \n",
        "return Max lenght of columns\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def retmax(dftrain):\n",
        "\n",
        "  stmax,sumax,jumax=0,0,0\n",
        "  for i in range(dftrain.shape[0]):\n",
        "\n",
        "    stmax=max(stmax,len(np.array(dftrain.loc[i,'statement'])))\n",
        "\n",
        "    sumax=max(sumax,len(np.array(dftrain.loc[i,'subject'])))\n",
        "\n",
        "    jumax=max(jumax,len(np.array(dftrain.loc[i,'justification'])))\n",
        "\n",
        "  return stmax,sumax,jumax\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GG5byD7ZbBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Convert Data Frame to Matrix 2D by Adding padding zeros to every columns that not have lenght not equal max\n",
        "lenght.\n",
        "take Data Frame list of Max Lenghts of Columns\n",
        "return Matrix after convert\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def convert2D(Xs,max_lens):\n",
        "   X_indices = np.zeros((Xs[0].shape[0], sum(max_lens)))\n",
        "   pls=0\n",
        "\n",
        "   for i in range(Xs[0].shape[0]):\n",
        "     pls=0\n",
        "     \n",
        "     for j in range(0,len(Xs[0][i])):\n",
        "       X_indices[i][j+pls]=Xs[0][i][j]\n",
        "     pls=max_lens[0]\n",
        "\n",
        "     for j in range(0,len(Xs[1][i])):\n",
        "       X_indices[i][j+pls+1]=Xs[1][i][j]\n",
        "     pls=max_lens[1]+max_lens[0]\n",
        "\n",
        "     for j in range(0,len(Xs[2][i])):\n",
        "       X_indices[i][j+pls+1]=Xs[2][i][j]\n",
        "   return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C7Ej5cQNHUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "convert labels to 0,1 True or False  \n",
        "\n",
        "\"\"\"\n",
        "convertlabel = {\n",
        "\t'pants-fire': 0,\n",
        "\t'false': 0,\n",
        "\t'barely-true': 0,\n",
        "\t'half-true': 1,\n",
        "\t'mostly-true': 1,\n",
        "\t'true': 1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvDjhEJ5AuqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "list of columns's Name \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "cols=['index','ID','label','statement','subject','speaker',\n",
        "      'speaker_job','state','party','barely_true',\n",
        "      'false','half_true','mostly_true','pants_on_fire',\n",
        "      'context','justification']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmu1XWkeJ3gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Call read_glove_vecs function and then call pretrained_embedding_layer to calc word Embedding of Words\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"/content/drive/My Drive/Datasets/Word Embedding/glove.6B.300d.txt\")\n",
        "word_embedding=pretrained_embedding_layer(word_to_vec_map, word_to_index)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI1wca7kAHbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Read Dataset (Data Frame)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dftrain=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/train.tsv\",sep=\"\\t\",header=None)\n",
        "dfval=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/val.tsv\",sep=\"\\t\",header=None)\n",
        "dftest=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/test.tsv\",sep=\"\\t\",header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8jZpxs8CIrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Add list cols to Data Frame columns\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dftrain.columns=cols\n",
        "dfval.columns=cols\n",
        "dftest.columns=cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCo9SEQ6SiQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Make Datasets have only statement,subject,justification and label \n",
        "important Feauters to Training\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dftrain=dftrain.loc[:,['statement','subject','justification','label']]\n",
        "dfval=dfval.loc[:,['statement','subject','justification','label']]\n",
        "dftest=dftest.loc[:,['statement','subject','justification','label']]\n",
        "dftrain=dftrain.append(dfval)\n",
        "dftrain=dftrain.append(dftest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiytTGihAnEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Drop NAN value and index column in Datasets \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dftrain=dftrain.dropna(axis=0)\n",
        "\n",
        "dftrain=dftrain.reset_index()\n",
        "\n",
        "dftrain=dftrain.drop(['index'],axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlwxZGVZMi54",
        "colab_type": "code",
        "outputId": "fc7bf1e5-8f51-4d14-86ba-ebcbcf3c600e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "\"\"\"  Show first two's row in dataset \"\"\"\n",
        "\n",
        "dftrain.head(2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>justification</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>That's a premise that he fails to back up. Ann...</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>Surovell said the decline of coal \"started whe...</td>\n",
              "      <td>half-true</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           statement  ...      label\n",
              "0  Says the Annies List political group supports ...  ...      false\n",
              "1  When did the decline of coal start? It started...  ...  half-true\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHJmQW1XNO-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Convert Labels dataset to 0,1 by Call convertlabel Function , \n",
        "sentence to indeces by call transfer_sent Function\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for i in range(dftrain.shape[0]):\n",
        "  dftrain.loc[i,'label']=convertlabel[dftrain.loc[i,'label']]\n",
        "  dftrain.loc[i,'statement']=transfer_sent(clean(dftrain.loc[i,'statement']),word_to_index)\n",
        "  dftrain.loc[i,'subject']=transfer_sent(clean(dftrain.loc[i,'subject']),word_to_index)\n",
        "  dftrain.loc[i,'justification']=transfer_sent(clean(dftrain.loc[i,'justification']),word_to_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ2rer47LEc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Take Labels two make only Target dataset and drop it in orignal dataset \"\"\"\n",
        "\n",
        "dftrainy=dftrain['label']\n",
        "\n",
        "dftrain=dftrain.drop(['label'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP-MvvSQU45O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Calc Max Lengths in every Columns by call retmax Function ,\n",
        "convert Data Frame to Matrix by convert2D Function\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "stmax,sumax,jumax=retmax(dftrain)\n",
        "Fulldata=np.array(convert2D([dftrain.statement,dftrain.subject,dftrain.justification],[410,30,1000]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGjc_T6CrFAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Split dataets to Training ,Validation and Testing Datasets \"\"\"\n",
        "\n",
        "dftrainy=list(dftrainy)\n",
        "training,trainingy=Fulldata[:8885],dftrainy[:8885]\n",
        "validation,validationy=Fulldata[8886:11425],dftrainy[8886:11425]\n",
        "testing,testingy=Fulldata[11426:],dftrainy[11426:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bInMSM2trKpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Convert every Datasets to Torch Datasets \"\"\"\n",
        "\n",
        "training=torch.from_numpy(training)\n",
        "trainingy = torch.tensor(trainingy) \n",
        "train_tensor = torch.utils.data.TensorDataset(training, trainingy)\n",
        "\n",
        "validation=torch.from_numpy(validation)\n",
        "validationy = torch.tensor(validationy) \n",
        "valid_tensor = torch.utils.data.TensorDataset(validation, validationy)\n",
        "\n",
        "testing=torch.from_numpy(testing)\n",
        "testingy = torch.tensor(testingy) \n",
        "test_tensor = torch.utils.data.TensorDataset(testing, testingy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT2PBtkUwPLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Create DataLoader to Every Datasets \"\"\"\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(dataset=train_tensor,batch_size=32,shuffle=True, num_workers=0)\n",
        "vali_loader=torch.utils.data.DataLoader(dataset=valid_tensor,batch_size=32,shuffle=True, num_workers=0)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_tensor,batch_size=32,shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2kIv7YUymWl",
        "colab_type": "code",
        "outputId": "c77f3f9b-d7c0-4860-f72a-950e96747f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "\"\"\"\n",
        "object from NN Model\n",
        "and print it\n",
        "\"\"\"\n",
        "vocab_size = len(word_to_index)+1\n",
        "embedding_dim = 300\n",
        "net = NN(vocab_size, embedding_dim,torch.from_numpy(word_embedding))\n",
        "print(net)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN(\n",
            "  (embedding): Embedding(400001, 300)\n",
            "  (lstm): LSTM(903, 128, num_layers=2, batch_first=True, dropout=0.6)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ewj9UOVymBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Learning rate 0.001\n",
        "create Binary Cross Entropy Loss function\n",
        "Create Adam optimizer to optimization parameters of NN ( Embedding , Linear Layers )\n",
        "\n",
        "\"\"\"\n",
        "lr=0.001\n",
        "\n",
        "net.cuda()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer1 = torch.optim.Adam(net.lstm.parameters(), lr=lr)\n",
        "\n",
        "optimizer2 = torch.optim.Adam(net.fc.parameters(), lr=lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1XfvzUylwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "e7754d7a-ffcf-450b-ddca-2a6e5f79cff2"
      },
      "source": [
        "\"\"\" 10 Number Epoch \"\"\"\n",
        "\n",
        "LtraininVis,LvalidVis=[],[]\n",
        "batch_size=32\n",
        "epochs = 25\n",
        "mnloss=np.Inf\n",
        "\n",
        "net.train() \n",
        "\n",
        "for e in range(epochs):\n",
        "\n",
        "  Taccuracy,Vaccuracy=[],[]\n",
        "  losses=[]\n",
        "  h = net.init_hidden(batch_size)\n",
        "  for inputs, labels in train_loader:\n",
        "\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()  \n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    net.zero_grad()\n",
        "    \n",
        "    output,h= net(inputs.long(),h)\n",
        "\n",
        "    Taccuracy.append(Accu(torch.round(output),labels))\n",
        "\n",
        "\n",
        "    loss = criterion(output.squeeze().float(), labels.float())\n",
        "\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
        "    optimizer1.step()\n",
        "    optimizer2.step()\n",
        "   \n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    net.eval()\n",
        "    val_losses = []\n",
        "    \n",
        "    val_h = net.init_hidden(batch_size)\n",
        "\n",
        "    for inputs, labels in vali_loader:\n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "      val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "      output,val_h= net(inputs.long(),val_h)\n",
        "\n",
        "      Vaccuracy.append(Accu(torch.round(output),labels))\n",
        "\n",
        "      val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "      val_losses.append(val_loss.item())\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    LtraininVis.append(np.mean(losses))\n",
        "    LvalidVis.append(np.mean(val_losses))\n",
        "    \n",
        "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "          \"Tarining Loss: {:.6f}...\".format(np.mean(losses)),\n",
        "          \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "          \"Val Accu:{:.6f}\".format(np.mean(Vaccuracy)),\n",
        "          \"Training Accu:{:.6f}\".format(np.mean(Taccuracy)))\n",
        "    if mnloss>np.mean(val_losses):\n",
        "      mnloss=np.mean(val_losses)\n",
        "      print(\"Saving Model......\")\n",
        "      torch.save(net.state_dict(),\"liar.pkl\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/25... Tarining Loss: 0.681180... Val Loss: 0.670641 Val Accu:0.571946 Training Accu:0.562591\n",
            "Saving Model......\n",
            "Epoch: 2/25... Tarining Loss: 0.668128... Val Loss: 0.666082 Val Accu:0.597408 Training Accu:0.592273\n",
            "Saving Model......\n",
            "Epoch: 3/25... Tarining Loss: 0.660568... Val Loss: 0.660028 Val Accu:0.613033 Training Accu:0.604745\n",
            "Saving Model......\n",
            "Epoch: 4/25... Tarining Loss: 0.655172... Val Loss: 0.657196 Val Accu:0.614986 Training Accu:0.611714\n",
            "Saving Model......\n",
            "Epoch: 5/25... Tarining Loss: 0.651382... Val Loss: 0.664944 Val Accu:0.603622 Training Accu:0.616885\n",
            "Epoch: 6/25... Tarining Loss: 0.645589... Val Loss: 0.657434 Val Accu:0.610369 Training Accu:0.629684\n",
            "Epoch: 7/25... Tarining Loss: 0.643023... Val Loss: 0.657314 Val Accu:0.617294 Training Accu:0.630658\n",
            "Epoch: 8/25... Tarining Loss: 0.633986... Val Loss: 0.659810 Val Accu:0.612251 Training Accu:0.641396\n",
            "Epoch: 9/25... Tarining Loss: 0.630732... Val Loss: 0.655887 Val Accu:0.621626 Training Accu:0.641396\n",
            "Saving Model......\n",
            "Epoch: 10/25... Tarining Loss: 0.625272... Val Loss: 0.664415 Val Accu:0.625178 Training Accu:0.647236\n",
            "Epoch: 11/25... Tarining Loss: 0.617676... Val Loss: 0.660808 Val Accu:0.615021 Training Accu:0.656962\n",
            "Epoch: 12/25... Tarining Loss: 0.608198... Val Loss: 0.670247 Val Accu:0.615838 Training Accu:0.669830\n",
            "Epoch: 13/25... Tarining Loss: 0.595735... Val Loss: 0.667703 Val Accu:0.626207 Training Accu:0.674894\n",
            "Epoch: 14/25... Tarining Loss: 0.591426... Val Loss: 0.677056 Val Accu:0.609908 Training Accu:0.687923\n",
            "Epoch: 15/25... Tarining Loss: 0.577062... Val Loss: 0.689132 Val Accu:0.616548 Training Accu:0.691868\n",
            "Epoch: 16/25... Tarining Loss: 0.569718... Val Loss: 0.670246 Val Accu:0.617401 Training Accu:0.703559\n",
            "Epoch: 17/25... Tarining Loss: 0.554827... Val Loss: 0.683513 Val Accu:0.608736 Training Accu:0.709516\n",
            "Epoch: 18/25... Tarining Loss: 0.544663... Val Loss: 0.696194 Val Accu:0.620526 Training Accu:0.721314\n",
            "Epoch: 19/25... Tarining Loss: 0.533897... Val Loss: 0.694621 Val Accu:0.618892 Training Accu:0.725703\n",
            "Epoch: 20/25... Tarining Loss: 0.526207... Val Loss: 0.704049 Val Accu:0.619389 Training Accu:0.740359\n",
            "Epoch: 21/25... Tarining Loss: 0.504524... Val Loss: 0.725744 Val Accu:0.609482 Training Accu:0.752907\n",
            "Epoch: 22/25... Tarining Loss: 0.493670... Val Loss: 0.734196 Val Accu:0.612997 Training Accu:0.754368\n",
            "Epoch: 23/25... Tarining Loss: 0.488761... Val Loss: 0.739568 Val Accu:0.602876 Training Accu:0.761011\n",
            "Epoch: 24/25... Tarining Loss: 0.465962... Val Loss: 0.744506 Val Accu:0.615874 Training Accu:0.778092\n",
            "Epoch: 25/25... Tarining Loss: 0.452826... Val Loss: 0.795732 Val Accu:0.616548 Training Accu:0.784050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkpuEG1o4sF6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "62b69f0f-f64e-48cb-d973-5905ed482ba9"
      },
      "source": [
        "\"\"\" Visualiaztion Learning Curve \"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n",
        "ax1.plot(LtraininVis,label='Training Loss')\n",
        "ax1.plot(LvalidVis,label='Validation Loss')\n",
        "\n",
        "ax1.legend(frameon=False)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc630420b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFpCAYAAADUTv+7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7MMPAkzMzMzy+B/hPCVR1/6hIYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud-mafcC5V_g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b7eca34-738c-4732-9724-1697a1a7cef3"
      },
      "source": [
        "\"\"\"Testing Model\"\"\"\n",
        "net.load_state_dict(torch.load(\"liar.pkl\"))\n",
        "testloss=[]\n",
        "testaccutacy=[]\n",
        "net.eval()\n",
        "\n",
        "val_h = net.init_hidden(batch_size)\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "      test_h = tuple([each.data for each in val_h])\n",
        "\n",
        "      output,val_h= net(inputs.long(),val_h)\n",
        "\n",
        "      testaccutacy.append(Accu(torch.round(output),labels))\n",
        "\n",
        "      test_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "      testloss.append(val_loss.item())\n",
        "\n",
        "\n",
        "print(\"Testing Accuracy \",np.mean(testaccutacy))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy  0.6114583333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af-Q3x2B95le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}