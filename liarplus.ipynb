{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "liarplus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smara97/FakeNews/blob/master/liarplus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg8QmECF_eHE",
        "colab_type": "code",
        "outputId": "7493e84d-eec8-437c-bed7-ff92b3f6461a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from google.colab import files,drive\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from torch.autograd import Variable\n",
        "from string import punctuation\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import nltk\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k39R3AAAADa8",
        "colab_type": "code",
        "outputId": "5a1d406e-a15e-4569-dfd0-aeed87d8c46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3HBqw4Onjsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Model to Training Data \n",
        "\n",
        "  Forward Function\n",
        "Take length of Embedding and Dim of it and Create Embedding Layer by torch Framework\n",
        "Create Neural Network Layer take 903 input and return 256 as output layer\n",
        "then bass output to non-activation function layer then add dropout to output\n",
        "bass output to Linear take 256 and return number of class 1 and then add sigmoid to output\n",
        "\n",
        "  Conv Function \n",
        "Take inputs ( self, inputs layer[batch size of training inputs*Featuers] (64,1440) )\n",
        "\n",
        "conv inputs layer from [64,1440] to [64,903]\n",
        "\n",
        "first 300s number repersent the vector sentence(Statement) of Embedding\n",
        "\n",
        "301 add Similarity of Statement Featuer (first [410] numbers of orignal input) and \n",
        "  Subject Featuer (second [30] numbers of orignal input)\n",
        "\n",
        "from 302 to 602 add number repersent the vector sentence(Subject) of Embedding\n",
        "\n",
        "602 add Similarity of Subject Featuer ([30] numbers of orignal input) and \n",
        "  Justification Featuer (second [1000] numbers of orignal input)\n",
        "\n",
        "from 602 to 902 add number repersent the vector sentence(Justifaction) of Embedding\n",
        "\n",
        "903 add Similarity of Justification Featuer ([100] numbers of orignal input) and \n",
        "  Statement Featuer (second [410] numbers of orignal input)\n",
        "\n",
        "and return [64,903]\n",
        "\n",
        "Then Pass output of Conv to Forward\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class NN(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "\n",
        "    super(NN, self).__init__()\n",
        "\n",
        "        \n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)   \n",
        "    \n",
        "\n",
        "    self.fc = nn.Linear(903, 256)\n",
        "    self.relu1=nn.ReLU()\n",
        "    self.dropout1=nn.Dropout(0.4)\n",
        "    self.fc2=nn.Linear(256,1)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    x = self.conv(x)\n",
        "\n",
        "    out = self.dropout1(self.relu1(self.fc(x)))\n",
        "    out=self.sig(self.fc2(out))\n",
        "    return out\n",
        "    \n",
        "  def conv(self,x):\n",
        "    batch=len(x)\n",
        "\n",
        "    ret=torch.zeros((batch,903)).cuda()\n",
        "    st=torch.zeros(300).cuda()\n",
        "    su=torch.zeros(300).cuda()\n",
        "    ju=torch.zeros(300).cuda()\n",
        "\n",
        "\n",
        "    for i in range(len(x)):\n",
        "      cnt=0\n",
        "      for j in range(0,411):\n",
        "        if (x[i][j]!=0):\n",
        "          cnt+=1\n",
        "          st+=self.embedding(x[i][j])\n",
        "      st/=cnt\n",
        "      cnt=0\n",
        "      for j in range(411,441):\n",
        "        if (x[i][j]!=0):\n",
        "          cnt+=1\n",
        "          su+=self.embedding(x[i][j])\n",
        "      su/=cnt\n",
        "\n",
        "      cnt=0\n",
        "      for j in range(411,1440):\n",
        "        if (x[i][j]!=0):\n",
        "          cnt+=1\n",
        "          ju+=self.embedding(x[i][j])\n",
        "      ju/=cnt\n",
        "\n",
        "      indx=0\n",
        "      for j in range(len(st)):\n",
        "        ret[i][indx]=st[j]\n",
        "        indx+=1\n",
        "      ret[i][indx]=simlarity(st,su)\n",
        "      indx+=1\n",
        "      for j in range(len(su)):\n",
        "        ret[i][indx]=su[j]\n",
        "        indx+=1\n",
        "\n",
        "      ret[i][indx]=simlarity(su,ju)\n",
        "      indx+=1\n",
        "\n",
        "      for j in range(len(ju)):\n",
        "        ret[i][indx]=ju[j]\n",
        "        indx+=1\n",
        "\n",
        "      ret[i][indx]=simlarity(st,ju)\n",
        "    return ret\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpKF7Bj6Hw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        " Read Glove File take url of file return the two dictionaries ( word to index and word to vector in embedding )\n",
        " and one list of index to word  \n",
        " (glove file url) --> words_to_index, index_to_words, word_to_vec_map\n",
        " \n",
        " \"\"\"\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r',encoding='UTF-8') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_YGt1t36NVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Word Embeddings of words take dictionary of word to embedding and word to index\n",
        "and return Embeddings Matrix [index,Embedding] \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    vocab_len = len(word_to_index) + 1\n",
        "    emb_matrix = np.zeros((vocab_len,300))\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "    return emb_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG5u2iTih9ID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Clean Text \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def clean(text):\n",
        "  text=text.lower()\n",
        "  stp=set(stopwords.words(\"english\"))\n",
        "  placesp = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "  removech= re.compile('[^0-9a-z #+_]')\n",
        "  st=WordNetLemmatizer()\n",
        "  text=re.sub(placesp,' ',text)\n",
        "  text=re.sub(removech,' ',text)\n",
        "  text=text.split()\n",
        "  text=[w for w in text if not w in stp]\n",
        "  text=[st.lemmatize(w) for w in text]\n",
        "  text=\" \".join(text)\n",
        "  text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyXAjL26LJxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Transfer sentence to indeces word in Embedding\n",
        "take text and word to index dictionary \n",
        "return list of indeces word in Embedding\n",
        "\n",
        "\"\"\"\n",
        "def transfer_sent(text,word_to_index):\n",
        "  text=text.split(' ')\n",
        "  ret=[]\n",
        "  for w in text:\n",
        "    if w in word_to_index and w !=\"\":\n",
        "      ret.append(word_to_index[w])\n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAFkt1gUUIkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Similarity of two Documnets \n",
        "take two documnets\n",
        "return The Similarity of documents\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def simlarity(dim1,dim2):\n",
        "  return (torch.dot(dim1,dim2)/(torch.sqrt(torch.sum(dim1**2))*torch.sqrt(torch.sum(dim2**2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owt9-oD4iutn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Accuracy of predict labels\n",
        "take predict labels and target labels\n",
        "return number of accept label in predict labels\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def Accu(pred,labels):\n",
        "  ret=0\n",
        "  for i in range(len(labels)):\n",
        "    if pred[i]==labels[i]:\n",
        "      ret+=1\n",
        "  return ret/len(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N6FmfbWRwwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "calculate the Max Length in every column in Data Frame \n",
        "take Data Frame \n",
        "return Max lenght of columns\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def retmax(dftrain):\n",
        "\n",
        "  stmax,sumax,jumax=0,0,0\n",
        "  for i in range(dftrain.shape[0]):\n",
        "\n",
        "    stmax=max(stmax,len(np.array(dftrain.loc[i,'statement'])))\n",
        "\n",
        "    sumax=max(sumax,len(np.array(dftrain.loc[i,'subject'])))\n",
        "\n",
        "    jumax=max(jumax,len(np.array(dftrain.loc[i,'justification'])))\n",
        "\n",
        "  return stmax,sumax,jumax\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GG5byD7ZbBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Convert Data Frame to Matrix 2D by Adding padding zeros to every columns that not have lenght not equal max\n",
        "lenght.\n",
        "take Data Frame list of Max Lenghts of Columns\n",
        "return Matrix after convert\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def convert2D(Xs,max_lens):\n",
        "   X_indices = np.zeros((Xs[0].shape[0], sum(max_lens)))\n",
        "   pls=0\n",
        "\n",
        "   for i in range(Xs[0].shape[0]):\n",
        "     pls=0\n",
        "     \n",
        "     for j in range(0,len(Xs[0][i])):\n",
        "       X_indices[i][j+pls]=Xs[0][i][j]\n",
        "     pls=max_lens[0]\n",
        "\n",
        "     for j in range(0,len(Xs[1][i])):\n",
        "       X_indices[i][j+pls+1]=Xs[1][i][j]\n",
        "     pls=max_lens[1]+max_lens[0]\n",
        "\n",
        "     for j in range(0,len(Xs[2][i])):\n",
        "       X_indices[i][j+pls+1]=Xs[2][i][j]\n",
        "   return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C7Ej5cQNHUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "convert labels to 0,1 True or False  \n",
        "\n",
        "\"\"\"\n",
        "convertlabel = {\n",
        "\t'pants-fire': 0,\n",
        "\t'false': 0,\n",
        "\t'barely-true': 0,\n",
        "\t'half-true': 1,\n",
        "\t'mostly-true': 1,\n",
        "\t'true': 1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvDjhEJ5AuqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "list of columns's Name \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "cols=['index','ID','label','statement','subject','speaker',\n",
        "      'speaker_job','state','party','barely_true',\n",
        "      'false','half_true','mostly_true','pants_on_fire',\n",
        "      'context','justification']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmu1XWkeJ3gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Call read_glove_vecs function and then call pretrained_embedding_layer to calc word Embedding of Words\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"/content/drive/My Drive/Datasets/Word Embedding/glove.6B.300d.txt\")\n",
        "word_embedding=pretrained_embedding_layer(word_to_vec_map, word_to_index)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI1wca7kAHbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Read Dataset (Data Frame)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dftrain=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/train.tsv\",sep=\"\\t\",header=None)\n",
        "dfval=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/val.tsv\",sep=\"\\t\",header=None)\n",
        "dftest=pd.read_csv(\"/content/drive/My Drive/Datasets/liar-plus/test.tsv\",sep=\"\\t\",header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8jZpxs8CIrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Add list cols to Data Frame columns\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dftrain.columns=cols\n",
        "dfval.columns=cols\n",
        "dftest.columns=cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCo9SEQ6SiQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Make Datasets have only statement,subject,justification and label \n",
        "important Feauters to Training\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dftrain=dftrain.loc[:,['statement','subject','justification','label']]\n",
        "dfval=dfval.loc[:,['statement','subject','justification','label']]\n",
        "dftest=dftest.loc[:,['statement','subject','justification','label']]\n",
        "dftrain=dftrain.append(dfval)\n",
        "dftrain=dftrain.append(dftest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiytTGihAnEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Drop NAN value and index column in Datasets \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "dftrain=dftrain.dropna(axis=0)\n",
        "\n",
        "dftrain=dftrain.reset_index()\n",
        "\n",
        "dftrain=dftrain.drop(['index'],axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlwxZGVZMi54",
        "colab_type": "code",
        "outputId": "cb0e3a6e-be08-4662-c64b-e332bfe85f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "\"\"\"  Show first two's row in dataset \"\"\"\n",
        "\n",
        "dftrain.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>justification</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>That's a premise that he fails to back up. Ann...</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>Surovell said the decline of coal \"started whe...</td>\n",
              "      <td>half-true</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           statement  ...      label\n",
              "0  Says the Annies List political group supports ...  ...      false\n",
              "1  When did the decline of coal start? It started...  ...  half-true\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHJmQW1XNO-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Convert Labels dataset to 0,1 by Call convertlabel Function , \n",
        "sentence to indeces by call transfer_sent Function\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for i in range(dftrain.shape[0]):\n",
        "  dftrain.loc[i,'label']=convertlabel[dftrain.loc[i,'label']]\n",
        "  dftrain.loc[i,'statement']=transfer_sent(clean(dftrain.loc[i,'statement']),word_to_index)\n",
        "  dftrain.loc[i,'subject']=transfer_sent(clean(dftrain.loc[i,'subject']),word_to_index)\n",
        "  dftrain.loc[i,'justification']=transfer_sent(clean(dftrain.loc[i,'justification']),word_to_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ2rer47LEc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Take Labels two make only Target dataset and drop it in orignal dataset \"\"\"\n",
        "\n",
        "dftrainy=dftrain['label']\n",
        "\n",
        "dftrain=dftrain.drop(['label'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP-MvvSQU45O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Calc Max Lengths in every Columns by call retmax Function ,\n",
        "convert Data Frame to Matrix by convert2D Function\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "stmax,sumax,jumax=retmax(dftrain)\n",
        "Fulldata=np.array(convert2D([dftrain.statement,dftrain.subject,dftrain.justification],[410,30,1000]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGjc_T6CrFAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Split dataets to Training ,Validation and Testing Datasets \"\"\"\n",
        "\n",
        "dftrainy=list(dftrainy)\n",
        "training,trainingy=Fulldata[:10153],dftrainy[:10153]\n",
        "validation,validationy=Fulldata[10154:11422],dftrainy[10154:11422]\n",
        "testing,testingy=Fulldata[11423:],dftrainy[11423:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bInMSM2trKpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Convert every Datasets to Torch Datasets \"\"\"\n",
        "\n",
        "training=torch.from_numpy(training)\n",
        "trainingy = torch.tensor(trainingy) \n",
        "train_tensor = torch.utils.data.TensorDataset(training, trainingy)\n",
        "\n",
        "validation=torch.from_numpy(validation)\n",
        "validationy = torch.tensor(validationy) \n",
        "valid_tensor = torch.utils.data.TensorDataset(validation, validationy)\n",
        "\n",
        "testing=torch.from_numpy(testing)\n",
        "testingy = torch.tensor(testingy) \n",
        "test_tensor = torch.utils.data.TensorDataset(testing, testingy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT2PBtkUwPLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Create DataLoader to Every Datasets \"\"\"\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(dataset=train_tensor,batch_size=64,shuffle=True, num_workers=0)\n",
        "vali_loader=torch.utils.data.DataLoader(dataset=valid_tensor,batch_size=64,shuffle=True, num_workers=0)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_tensor,batch_size=64,shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2kIv7YUymWl",
        "colab_type": "code",
        "outputId": "5cc94e86-2178-4bc0-88e1-5d9198569f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "\"\"\"\n",
        "object from NN Model\n",
        "and print it\n",
        "\"\"\"\n",
        "vocab_size = len(word_to_index)+1\n",
        "embedding_dim = 300\n",
        "net = NN(vocab_size, embedding_dim)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN(\n",
            "  (embedding): Embedding(400001, 300)\n",
            "  (fc): Linear(in_features=903, out_features=256, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (dropout1): Dropout(p=0.4, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ewj9UOVymBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Learning rate 0.01\n",
        "create Binary Cross Entropy Loss function\n",
        "Create Adam optimizer to optimization parameters of NN ( Embedding , Linear Layers )\n",
        "\n",
        "\"\"\"\n",
        "lr=0.01\n",
        "\n",
        "net.cuda()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1XfvzUylwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 4 Number Epoch \"\"\"\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "net.train() \n",
        "\n",
        "for e in range(epochs):\n",
        "\n",
        "  Taccuracy,Vaccuracy=[],[]\n",
        "  losses=[]\n",
        "  \n",
        "  for inputs, labels in train_loader:\n",
        "\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()  \n",
        "\n",
        "   \n",
        "    net.zero_grad()\n",
        "    \n",
        "    output= net(inputs.long())\n",
        "\n",
        "    Taccuracy.append(Accu(torch.round(output),labels))\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    loss = criterion(output.squeeze().float(), labels.float())\n",
        "\n",
        "    loss = Variable(loss, requires_grad = True)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  print(\"Epoch: {}/{}...\".format(e+1, epochs),\"Training Finished\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    net.eval()\n",
        "    val_losses = []\n",
        "  \n",
        "    for inputs, labels in vali_loader:\n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "      output= net(inputs.long())\n",
        "\n",
        "      Vaccuracy.append(Accu(torch.round(output),labels))\n",
        "\n",
        "      val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "      val_losses.append(val_loss.item())\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "          \"Tarining Loss: {:.6f}...\".format(np.mean(losses)),\n",
        "          \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "          \"Val Accu:{:.6f}\".format(np.mean(Vaccuracy)),\n",
        "          \"Training Accu:{:.6f}\".format(np.mean(Taccuracy)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHhL5N2zfBWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}